{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a080f8",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week13.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Advanced machine learning and data analysis for the physical sciences -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e6515",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Advanced machine learning and data analysis for the physical sciences\n",
    "**Morten Hjorth-Jensen**, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway and Department of Physics and Astronomy and Facility for Rare Isotope Beams, Michigan State University, East Lansing, Michigan, USA\n",
    "\n",
    "Date: **April 16, 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d54e8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plans for the week April 15-19, 2024\n",
    "\n",
    "**Deep generative models.**\n",
    "\n",
    "1. Finalizing discussion of Boltzmann machines, implementations using TensorFlow and Pytorch\n",
    "\n",
    "2. Discussion of other energy-based models and Langevin sampling\n",
    "\n",
    "3. Variational Autoencoders (VAE), mathematics\n",
    "\n",
    "4. [Video of lecture](https://youtu.be/rw-NBN293o4)\n",
    "\n",
    "5. [Whiteboard notes](https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/HandwrittenNotes/2024/NotesApril16.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdee6cd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Readings\n",
    "1. Reading recommendation: Goodfellow et al, for VAEs see sections 20.10-20.11\n",
    "\n",
    "2. To create Boltzmann machine using Keras, see Babcock and Bali chapter 4, see <https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_4/models/rbm.py>\n",
    "\n",
    "3. See Foster, chapter 7 on energy-based models at <https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/tree/main/notebooks/07_ebm/01_ebm>\n",
    "\n",
    "<!-- todo: add about Langevin sampling, see <https://www.lyndonduong.com/sgmcmc/> -->\n",
    "<!-- code for VAEs applied to MNIST and CIFAR perhaps -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e7a71a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reminder from last week and layout of lecture this week\n",
    "\n",
    "1. We will present first a reminder from last week, see for example the jupyter-notebook at <https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week12/ipynb/week12.ipynb>\n",
    "\n",
    "2. We will then discuss codes as well as other energy-based models and Langevin sampling instead of Gibbs or Metropolis sampling.\n",
    "\n",
    "3. Thereafter we start our discussions of Variational autoencoders and Generalized adversarial networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed025ba",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Code for RBMs using PyTorch for a binary-binary RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a6be7ab",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "datasets.MNIST('./data',\n",
    "    train=True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor()])\n",
    "     ),\n",
    "     batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "datasets.MNIST('./data',\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "    ),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "\n",
    "class RBM(nn.Module):\n",
    "   def __init__(self,\n",
    "               n_vis=784,\n",
    "               n_hin=500,\n",
    "               k=5):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(n_hin,n_vis)*1e-2)\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_vis))\n",
    "        self.h_bias = nn.Parameter(torch.zeros(n_hin))\n",
    "        self.k = k\n",
    "    \n",
    "   def sample_from_p(self,p):\n",
    "       return F.relu(torch.sign(p - Variable(torch.rand(p.size()))))\n",
    "    \n",
    "   def v_to_h(self,v):\n",
    "        p_h = F.sigmoid(F.linear(v,self.W,self.h_bias))\n",
    "        sample_h = self.sample_from_p(p_h)\n",
    "        return p_h,sample_h\n",
    "    \n",
    "   def h_to_v(self,h):\n",
    "        p_v = F.sigmoid(F.linear(h,self.W.t(),self.v_bias))\n",
    "        sample_v = self.sample_from_p(p_v)\n",
    "        return p_v,sample_v\n",
    "        \n",
    "   def forward(self,v):\n",
    "        pre_h1,h1 = self.v_to_h(v)\n",
    "        \n",
    "        h_ = h1\n",
    "        for _ in range(self.k):\n",
    "            pre_v_,v_ = self.h_to_v(h_)\n",
    "            pre_h_,h_ = self.v_to_h(v_)\n",
    "        \n",
    "        return v,v_\n",
    "    \n",
    "   def free_energy(self,v):\n",
    "        vbias_term = v.mv(self.v_bias)\n",
    "        wx_b = F.linear(v,self.W,self.h_bias)\n",
    "        hidden_term = wx_b.exp().add(1).log().sum(1)\n",
    "        return (-hidden_term - vbias_term).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rbm = RBM(k=1)\n",
    "train_op = optim.SGD(rbm.parameters(),0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss_ = []\n",
    "    for _, (data,target) in enumerate(train_loader):\n",
    "        data = Variable(data.view(-1,784))\n",
    "        sample_data = data.bernoulli()\n",
    "        \n",
    "        v,v1 = rbm(sample_data)\n",
    "        loss = rbm.free_energy(v) - rbm.free_energy(v1)\n",
    "        loss_.append(loss.data)\n",
    "        train_op.zero_grad()\n",
    "        loss.backward()\n",
    "        train_op.step()\n",
    "\n",
    "    print(\"Training loss for {} epoch: {}\".format(epoch, np.mean(loss_)))\n",
    "\n",
    "\n",
    "def show_adn_save(file_name,img):\n",
    "    npimg = np.transpose(img.numpy(),(1,2,0))\n",
    "    f = \"./%s.png\" % file_name\n",
    "    plt.imshow(npimg)\n",
    "    plt.imsave(f,npimg)\n",
    "\n",
    "show_adn_save(\"real\",make_grid(v.view(32,1,28,28).data))\n",
    "show_adn_save(\"generate\",make_grid(v1.view(32,1,28,28).data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e869b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RBM using TensorFlow and Keras\n",
    "\n",
    "1. To create Boltzmann machine using Keras, see Babcock and Bali chapter 4, see <https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_4/models/rbm.py>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3f7b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Codes for Energy-based models\n",
    "\n",
    "See discussions in Foster, chapter 7 on energy-based models at <https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/tree/main/notebooks/07_ebm/01_ebm>\n",
    "\n",
    "That notebook is based on a recent article by Du and Mordatch, **Implicit generation and modeling with energy-based models**, see <https://arxiv.org/pdf/1903.08689.pdf.>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750ee3e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Langevin sampling\n",
    "\n",
    "Also called Stochastic gradient Langevin dynamics (SGLD), is sampling\n",
    "technique composed of characteristics from Stochastic gradient descent\n",
    "(SGD) and Langevin dynamics, a mathematical extension of the Langevin\n",
    "equation.  The SGLD is an iterative\n",
    "optimization algorithm which uses minibatching to create a stochastic\n",
    "gradient estimator, as used in SGD to optimize a differentiable\n",
    "objective function.\n",
    "\n",
    "Unlike traditional SGD, SGLD can be used for\n",
    "Bayesian learning as a sampling method. SGLD may be viewed as Langevin\n",
    "dynamics applied to posterior distributions, but the key difference is\n",
    "that the likelihood gradient terms are minibatched, like in SGD. SGLD,\n",
    "like Langevin dynamics, produces samples from a posterior distribution\n",
    "of parameters based on available data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1f367",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on the SGLD\n",
    "\n",
    "The SGLD uses the probability $p(\\theta)$ (note that we limit\n",
    "ourselves to just a variable $\\theta$) and updates the **log** of this\n",
    "probability by initializing it through some random prior distribution,\n",
    "normally just a uniform distribution which takes values between\n",
    "$\\theta\\in [-1,1]$,\n",
    "\n",
    "The update is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31a4bf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{i+1}=\\theta_{i}+\\eta \\nabla_{\\theta} \\log{p(\\theta_{i})}+\\sqrt{\\eta}w_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08b03b",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $w_i\\sim N(0,1)$ are normally distributed with mean zero and\n",
    "variance one and $i=0,1,\\dots,k$, with $k$ the final number of\n",
    "iterations.  The parameter $\\eta$ is the learning rate. The term\n",
    "$\\sqrt{\\eta}w_i$ introduces **noise** in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1d018",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Code example of Langevin Sampling\n",
    "\n",
    "In our calculations the gradient is calculated using the model we have\n",
    "for the probability distribution. For an energy-based model this gives\n",
    "us a derivative which involves the so-called positive and negative\n",
    "phases discussed last week.\n",
    "\n",
    "Read more about Langevin sampling at for example\n",
    "<https://www.lyndonduong.com/sgmcmc/>. This site contains a nice\n",
    "example of a PyTorch code which implements Langevin sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be28079",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Theory of Variational Autoencoders\n",
    "\n",
    "Let us remind ourself about what an autoencoder is, see the jupyter-notebook at <https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week10/ipynb/week10.ipynb>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec99b4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Autoencoder again\n",
    "\n",
    "Autoencoders are neural networks where the outputs are its own\n",
    "inputs. They are split into an **encoder part**\n",
    "which maps the input $\\boldsymbol{x}$ via a function $f(\\boldsymbol{x},\\boldsymbol{W})$ (this\n",
    "is the encoder part) to a **so-called code part** (or intermediate part)\n",
    "with the result $\\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c5d1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{h} = f(\\boldsymbol{x},\\boldsymbol{W})),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f663ee",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\boldsymbol{W}$ are the weights to be determined.  The **decoder** parts maps, via its own parameters (weights given by the matrix $\\boldsymbol{V}$ and its own biases) to \n",
    "the final ouput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132b5d5f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{x}} = g(\\boldsymbol{h},\\boldsymbol{V})).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee9b31",
   "metadata": {
    "editable": true
   },
   "source": [
    "The goal is to minimize the construction error, often done by optimizing the means squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273dfac",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Schematic image of an Autoencoder\n",
    "\n",
    "<!-- dom:FIGURE: [figures/ae1.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/ae1.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194f29ac",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematics of Variational Autoencoders\n",
    "\n",
    "We have defined earlier a probability (marginal) distribution with hidden variables $\\boldsymbol{h}$ and parameters $\\boldsymbol{\\Theta}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86061432",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x};\\boldsymbol{\\Theta}) = \\int d\\boldsymbol{h}p(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550f513",
   "metadata": {
    "editable": true
   },
   "source": [
    "for continuous variables $\\boldsymbol{h}$ and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128e3f0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x};\\boldsymbol{\\Theta}) = \\sum_{\\boldsymbol{h}}p(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0b33f",
   "metadata": {
    "editable": true
   },
   "source": [
    "for discrete stochastic events $\\boldsymbol{h}$. The variables $\\boldsymbol{h}$ are normally called the **latent variables** in the theory of autoencoders. We will also call then for that here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72741c6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using the conditional probability\n",
    "\n",
    "Using the the definition of the conditional probabilities $p(\\boldsymbol{x}\\vert\\boldsymbol{h};\\boldsymbol{\\Theta})$, $p(\\boldsymbol{h}\\vert\\boldsymbol{x};\\boldsymbol{\\Theta})$ and \n",
    "and the prior $p(\\boldsymbol{h})$, we can rewrite the above equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd0d2b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x};\\boldsymbol{\\Theta}) = \\sum_{\\boldsymbol{h}}p(\\boldsymbol{x}\\vert\\boldsymbol{h};\\boldsymbol{\\Theta})p(\\boldsymbol{h}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb94136",
   "metadata": {
    "editable": true
   },
   "source": [
    "which allows us to make the dependence of $\\boldsymbol{x}$ on $\\boldsymbol{h}$\n",
    "explicit by using the law of total probability. The intuition behind\n",
    "this approach for finding the marginal probability for $\\boldsymbol{x}$ is to\n",
    "optimize the above equations with respect to the parameters\n",
    "$\\boldsymbol{\\Theta}$.  This is done normally by maximizing the probability,\n",
    "the so-called maximum-likelihood approach discussed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f5eab",
   "metadata": {
    "editable": true
   },
   "source": [
    "## VAEs versus autoencoders\n",
    "\n",
    "This trained probability is assumed to be able to produce similar\n",
    "samples as the input.  In VAEs it is then common to compare via for\n",
    "example the mean-squared error or the cross-entropy the predicted\n",
    "values with the input values.  Compared with autoencoders, we are now\n",
    "producing a probability instead of a functions which mimicks the\n",
    "input.\n",
    "\n",
    "In VAEs, the choice of this output distribution is often Gaussian,\n",
    "meaning that the conditional probability is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b60d7a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x}\\vert\\boldsymbol{h};\\boldsymbol{\\Theta})=N(\\boldsymbol{x}\\vert f(\\boldsymbol{h};\\boldsymbol{\\Theta}), \\sigma^2\\times \\boldsymbol{I}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b67a2e",
   "metadata": {
    "editable": true
   },
   "source": [
    "with mean value given by the function $f(\\boldsymbol{h};\\boldsymbol{\\Theta})$ and a\n",
    "diagonal covariance matrix multiplied by a parameter $\\sigma^2$ which\n",
    "is treated as a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c300a591",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "By having a Gaussian distribution, we can use gradient descent (or any\n",
    "other optimization technique) to increase $p(\\boldsymbol{x};\\boldsymbol{\\Theta})$ by\n",
    "making $f(\\boldsymbol{h};\\boldsymbol{\\Theta})$ approach $\\boldsymbol{x}$ for some $\\boldsymbol{h}$,\n",
    "gradually making the training data more likely under the generative\n",
    "model. The important property is simply that the marginal probability\n",
    "can be computed, and it is continuous in $\\boldsymbol{\\Theta}$.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda7d79",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Are VAEs just modified autoencoders?\n",
    "\n",
    "The mathematical basis of VAEs actually has relatively little to do\n",
    "with classical autoencoders, for example the sparse autoencoders or\n",
    "denoising autoencoders discussed earlier.\n",
    "\n",
    "VAEs approximately maximize the probability equation discussed\n",
    "above. They are called autoencoders only because the final training\n",
    "objective that derives from this setup does have an encoder and a\n",
    "decoder, and resembles a traditional autoencoder. Unlike sparse\n",
    "autoencoders, there are generally no tuning parameters analogous to\n",
    "the sparsity penalties. And unlike sparse and denoising autoencoders,\n",
    "we can sample directly from $p(\\boldsymbol{x})$ without performing Markov\n",
    "Chain Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9d330",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Training VAEs\n",
    "\n",
    "To solve the integral or sum for $p(\\boldsymbol{x})$, there are two problems\n",
    "that VAEs must deal with: how to define the latent variables $\\boldsymbol{h}$,\n",
    "that is decide what information they represent, and how to deal with\n",
    "the integral over $\\boldsymbol{h}$.  VAEs give a definite answer to both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874020c2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Kullback-Leibler relative entropy (notation to be updated)\n",
    "\n",
    "When the goal of the training is to approximate a probability\n",
    "distribution, as it is in generative modeling, another relevant\n",
    "measure is the **Kullback-Leibler divergence**, also known as the\n",
    "relative entropy or Shannon entropy. It is a non-symmetric measure of the\n",
    "dissimilarity between two probability density functions $p$ and\n",
    "$q$. If $p$ is the unkown probability which we approximate with $q$,\n",
    "we can measure the difference by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5115d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\t\\text{KL}(p||q) = \\int_{-\\infty}^{\\infty} p (\\boldsymbol{x}) \\log \\frac{p(\\boldsymbol{x})}{q(\\boldsymbol{x})}  d\\boldsymbol{x}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ff4c4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Kullback-Leibler divergence and RBMs\n",
    "\n",
    "Thus, the Kullback-Leibler divergence between the distribution of the\n",
    "training data $f(\\boldsymbol{x})$ and the model marginal distribution $p(\\boldsymbol{x};\\boldsymbol{\\Theta})$ from an RBM is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea4d43",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\t\\text{KL} (f(\\boldsymbol{x})|| p(\\boldsymbol{x}| \\boldsymbol{\\Theta})) =& \\int_{-\\infty}^{\\infty}\n",
    "\tf (\\boldsymbol{x}) \\log \\frac{f(\\boldsymbol{x})}{p(\\boldsymbol{x}; \\boldsymbol{\\Theta})} d\\boldsymbol{x} \\\\\n",
    "\t=& \\int_{-\\infty}^{\\infty} f(\\boldsymbol{x}) \\log f(\\boldsymbol{x}) d\\boldsymbol{x} - \\int_{-\\infty}^{\\infty} f(\\boldsymbol{x}) \\log\n",
    "\tp(\\boldsymbol{x};\\boldsymbol{\\Theta}) d\\boldsymbol{x} \\\\\n",
    "\t%=& \\mathbb{E}_{f(\\boldsymbol{x})} (\\log f(\\boldsymbol{x})) - \\mathbb{E}_{f(\\boldsymbol{x})} (\\log p(\\boldsymbol{x}; \\boldsymbol{\\Theta}))\n",
    "\t=& \\langle \\log f(\\boldsymbol{x}) \\rangle_{f(\\boldsymbol{x})} - \\langle \\log p(\\boldsymbol{x};\\boldsymbol{\\Theta}) \\rangle_{f(\\boldsymbol{x})} \\\\\n",
    "\t=& \\langle \\log f(\\boldsymbol{x}) \\rangle_{data} + \\langle E(\\boldsymbol{x}) \\rangle_{data} + \\log Z.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4bd2a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Maximizing log-likelihood\n",
    "\n",
    "The first term is constant with respect to $\\boldsymbol{\\Theta}$ since\n",
    "$f(\\boldsymbol{x})$ is independent of $\\boldsymbol{\\Theta}$. Thus the Kullback-Leibler\n",
    "divergence is minimal when the second term is minimal. The second term\n",
    "is the log-likelihood cost function, hence minimizing the\n",
    "Kullback-Leibler divergence is equivalent to maximizing the\n",
    "log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f7c663",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back to VAEs\n",
    "\n",
    "We want to train the marginal probability with some latent variables $\\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35ab7d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "p(\\boldsymbol{x};\\boldsymbol{\\Theta}) = \\int d\\boldsymbol{h}p(\\boldsymbol{x},\\boldsymbol{h};\\boldsymbol{\\Theta}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487b137f",
   "metadata": {
    "editable": true
   },
   "source": [
    "for the continuous version (see previous slides for the discrete variant)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da4cc0f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using the KL divergence\n",
    "\n",
    "In practice, for most $\\boldsymbol{h}$, $p(\\boldsymbol{x}\\vert \\boldsymbol{h}; \\boldsymbol{\\Theta})$\n",
    "will be nearly zero, and hence contribute almost nothing to our\n",
    "estimate of $p(\\boldsymbol{x})$.\n",
    "\n",
    "The key idea behind the variational autoencoder is to attempt to\n",
    "sample values of $\\boldsymbol{h}$ that are likely to have produced $\\boldsymbol{x}$,\n",
    "and compute $p(\\boldsymbol{x})$ just from those.\n",
    "\n",
    "This means that we need a new function $Q(\\boldsymbol{h}|\\boldsymbol{x})$ which can\n",
    "take a value of $\\boldsymbol{x}$ and give us a distribution over $\\boldsymbol{h}$\n",
    "values that are likely to produce $\\boldsymbol{x}$.  Hopefully the space of\n",
    "$\\boldsymbol{h}$ values that are likely under $Q$ will be much smaller than\n",
    "the space of all $\\boldsymbol{h}$'s that are likely under the prior\n",
    "$p(\\boldsymbol{h})$.  This lets us, for example, compute $E_{\\boldsymbol{h}\\sim\n",
    "Q}p(\\boldsymbol{x}\\vert \\boldsymbol{h})$ relatively easily. Note that we drop\n",
    "$\\boldsymbol{\\Theta}$ from here and for notational simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15ba0d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Kullback-Leibler again\n",
    "\n",
    "However, if $\\boldsymbol{h}$ is sampled from an arbitrary distribution with\n",
    "PDF $Q(\\boldsymbol{h})$, which is not $\\mathcal{N}(0,I)$, then how does that\n",
    "help us optimize $p(\\boldsymbol{x})$?\n",
    "\n",
    "The first thing we need to do is relate\n",
    "$E_{\\boldsymbol{h}\\sim Q}P(\\boldsymbol{x}\\vert \\boldsymbol{h})$ and $p(\\boldsymbol{x})$.  We will see where $Q$ comes from later.\n",
    "\n",
    "The relationship between $E_{\\boldsymbol{h}\\sim Q}p(\\boldsymbol{x}\\vert \\boldsymbol{h})$ and $p(\\boldsymbol{x})$ is one of the cornerstones of variational Bayesian methods.\n",
    "We begin with the definition of Kullback-Leibler divergence (KL divergence or $\\mathcal{D}$) between $p(\\boldsymbol{h}\\vert \\boldsymbol{x})$ and $Q(\\boldsymbol{h})$, for some arbitrary $Q$ (which may or may not depend on $\\boldsymbol{x}$):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4077fff",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathcal{D}\\left[Q(\\boldsymbol{h})\\|p(\\boldsymbol{h}|\\boldsymbol{x})\\right]=E_{\\boldsymbol{h}\\sim Q}\\left[\\log Q(\\boldsymbol{h}) - \\log p(\\boldsymbol{h}|\\boldsymbol{x}) \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ee224",
   "metadata": {
    "editable": true
   },
   "source": [
    "## And applying Bayes rule\n",
    "\n",
    "We can get both $p(\\boldsymbol{x})$ and $p(\\boldsymbol{x}\\vert \\boldsymbol{h})$ into this equation by applying Bayes rule to $p(\\boldsymbol{h}|\\boldsymbol{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f188f0d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathcal{D}\\left[Q(\\boldsymbol{h})\\|p(\\boldsymbol{h}\\vert \\boldsymbol{x})\\right]=E_{\\boldsymbol{h}\\sim Q}\\left[\\log Q(\\boldsymbol{h}) - \\log p(\\boldsymbol{x}|\\boldsymbol{h}) - \\log p(\\boldsymbol{h}) \\right] + \\log p(\\boldsymbol{x}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ac385",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, $\\log p(\\boldsymbol{x})$ comes out of the expectation because it does not depend on $\\boldsymbol{h}$.\n",
    "Negating both sides, rearranging, and contracting part of $E_{\\boldsymbol{h}\\sim Q}$ into a KL-divergence terms yields:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffbfdd8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\log p(\\boldsymbol{x}) - \\mathcal{D}\\left[Q(\\boldsymbol{h})\\|p(\\boldsymbol{h}\\vert \\boldsymbol{x})\\right]=E_{\\boldsymbol{h}\\sim Q}\\left[\\log p(\\boldsymbol{x}\\vert\\boldsymbol{h})  \\right] - \\mathcal{D}\\left[Q(\\boldsymbol{h})\\|P(\\boldsymbol{h})\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eccb2b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Rearranging\n",
    "\n",
    "Using Bayes rule we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c7dc0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E_{\\boldsymbol{h}\\sim Q}\\left[\\log p(y_i|\\boldsymbol{h},x_i)\\right]=E_{\\boldsymbol{h}\\sim Q}\\left[\\log p(\\boldsymbol{h}|y_i,x_i) - \\log p(\\boldsymbol{h}|x_i) + \\log p(y_i|x_i) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983dc1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "Rearranging the terms and subtracting $E_{\\boldsymbol{h}\\sim Q}\\log Q(\\boldsymbol{h})$ from both sides gives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c3aa6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "\\log P(y_i|x_i) - E_{\\boldsymbol{h}\\sim Q}\\left[\\log Q(\\boldsymbol{h})-\\log p(\\boldsymbol{h}|x_i,y_i)\\right]=\\hspace{10em}\\\\\n",
    "\\hspace{10em}E_{\\boldsymbol{h}\\sim Q}\\left[\\log p(y_i|\\boldsymbol{h},x_i)+\\log p(\\boldsymbol{h}|x_i)-\\log Q(\\boldsymbol{h})\\right]\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4a858",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that $\\boldsymbol{x}$ is fixed, and $Q$ can be \\textit{any} distribution, not\n",
    "just a distribution which does a good job mapping $\\boldsymbol{x}$ to the $\\boldsymbol{h}$'s\n",
    "that can produce $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a97ab9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Inferring the probability\n",
    "\n",
    "Since we are interested in inferring $p(\\boldsymbol{x})$, it makes sense to\n",
    "construct a $Q$ which \\textit{does} depend on $\\boldsymbol{x}$, and in particular,\n",
    "one which makes $\\mathcal{D}\\left[Q(\\boldsymbol{h})\\|p(\\boldsymbol{h}|\\boldsymbol{x})\\right]$ small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f551ef",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\log p(\\boldsymbol{x}) - \\mathcal{D}\\left[Q(\\boldsymbol{h}|\\boldsymbol{x})\\|p(\\boldsymbol{h}|\\boldsymbol{x})\\right]=E_{\\boldsymbol{h}\\sim Q}\\left[\\log p(\\boldsymbol{x}|\\boldsymbol{h})  \\right] - \\mathcal{D}\\left[Q(\\boldsymbol{h}|\\boldsymbol{x})\\|p(\\boldsymbol{h})\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5c8dd",
   "metadata": {
    "editable": true
   },
   "source": [
    "Hence, during training, it makes sense to choose a $Q$ which will make\n",
    "$E_{\\boldsymbol{h}\\sim Q}[\\log Q(\\boldsymbol{h})-$ $\\log p(\\boldsymbol{h}|x_i,y_i)]$ (a\n",
    "$\\mathcal{D}$-divergence) small, such that the right hand side is a\n",
    "close approximation to $\\log p(y_i|y_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b06bb4b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Central equation of VAEs\n",
    "\n",
    "This equation serves as the core of the variational autoencoder, and\n",
    "it is worth spending some time thinking about what it means.\n",
    "\n",
    "1. The left hand side has the quantity we want to maximize, namely $\\log p(\\boldsymbol{x})$ plus an error term.\n",
    "\n",
    "2. The right hand side is something we can optimize via stochastic gradient descent given the right choice of $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff59f5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Setting up SGD\n",
    "So how can we perform stochastic gradient descent?\n",
    "\n",
    "First we need to be a bit more specific about the form that $Q(\\boldsymbol{h}|\\boldsymbol{x})$\n",
    "will take.  The usual choice is to say that\n",
    "$Q(\\boldsymbol{h}|\\boldsymbol{x})=\\mathcal{N}(\\boldsymbol{h}|\\mu(\\boldsymbol{x};\\vartheta),\\Sigma(;\\vartheta))$, where\n",
    "$\\mu$ and $\\Sigma$ are arbitrary deterministic functions with\n",
    "parameters $\\vartheta$ that can be learned from data (we will omit\n",
    "$\\vartheta$ in later equations).  In practice, $\\mu$ and $\\Sigma$ are\n",
    "again implemented via neural networks, and $\\Sigma$ is constrained to\n",
    "be a diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbebe5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on the SGD\n",
    "\n",
    "The name variational \"autoencoder\" comes from\n",
    "the fact that $\\mu$ and $\\Sigma$ are \"encoding\" $\\boldsymbol{x}$ into the latent\n",
    "space $\\boldsymbol{h}$.  The advantages of this choice are computational, as they\n",
    "make it clear how to compute the right hand side.  The last\n",
    "term---$\\mathcal{D}\\left[Q(\\boldsymbol{h}|\\boldsymbol{x})\\|p(\\boldsymbol{h})\\right]$---is now a KL-divergence\n",
    "between two multivariate Gaussian distributions, which can be computed\n",
    "in closed form as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ec03c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    " \\mathcal{D}[\\mathcal{N}(\\mu_0,\\Sigma_0) \\| \\mathcal{N}(\\mu_1,\\Sigma_1)] = \\hspace{20em}\\\\\n",
    "  \\hspace{5em}\\frac{ 1 }{ 2 } \\left( \\mathrm{tr} \\left( \\Sigma_1^{-1} \\Sigma_0 \\right) + \\left( \\mu_1 - \\mu_0\\right)^\\top \\Sigma_1^{-1} ( \\mu_1 - \\mu_0 ) - k + \\log \\left( \\frac{ \\det \\Sigma_1 }{ \\det \\Sigma_0  } \\right)  \\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d168e0",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $k$ is the dimensionality of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ad1cc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simplification\n",
    "In our case, this simplifies to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d01a2d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    " \\mathcal{D}[\\mathcal{N}(\\mu(X),\\Sigma(X)) \\| \\mathcal{N}(0,I)] = \\hspace{20em}\\\\\n",
    "\\hspace{6em}\\frac{ 1 }{ 2 } \\left( \\mathrm{tr} \\left( \\Sigma(X) \\right) + \\left( \\mu(X)\\right)^\\top ( \\mu(X) ) - k - \\log\\det\\left(  \\Sigma(X)  \\right)  \\right).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ff8ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Terms to compute\n",
    "\n",
    "The first term on the right hand side is a bit more tricky.\n",
    "We could use sampling to estimate $E_{z\\sim Q}\\left[\\log P(X|z)  \\right]$, but getting a good estimate would require passing many samples of $z$ through $f$, which would be expensive.\n",
    "Hence, as is standard in stochastic gradient descent, we take one sample of $z$ and treat $\\log P(X|z)$ for that $z$ as an approximation of $E_{z\\sim Q}\\left[\\log P(X|z)  \\right]$.\n",
    "After all, we are already doing stochastic gradient descent over different values of $X$ sampled from a dataset $D$.\n",
    "The full equation we want to optimize is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ba73b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "    E_{X\\sim D}\\left[\\log P(X) - \\mathcal{D}\\left[Q(z|X)\\|P(z|X)\\right]\\right]=\\hspace{16em}\\\\\n",
    "\\hspace{10em}E_{X\\sim D}\\left[E_{z\\sim Q}\\left[\\log P(X|z)  \\right] - \\mathcal{D}\\left[Q(z|X)\\|P(z)\\right]\\right].\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40830c35",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Computing the gradients\n",
    "\n",
    "If we take the gradient of this equation, the gradient symbol can be moved into the expectations.\n",
    "Therefore, we can sample a single value of $X$ and a single value of $z$ from the distribution $Q(z|X)$, and compute the gradient of:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86bbee2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\log P(X|z)-\\mathcal{D}\\left[Q(z|X)\\|P(z)\\right].\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f06f4",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can then average the gradient of this function over arbitrarily many samples of $X$ and $z$, and the result converges to the gradient.\n",
    "\n",
    "There is, however, a significant problem\n",
    "$E_{z\\sim Q}\\left[\\log P(X|z)  \\right]$ depends not just on the parameters of $P$, but also on the parameters of $Q$.\n",
    "\n",
    "In order to make VAEs work, it is essential to drive $Q$ to produce codes for $X$ that $P$ can reliably decode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc18763",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E_{X\\sim D}\\left[E_{\\epsilon\\sim\\mathcal{N}(0,I)}[\\log P(X|z=\\mu(X)+\\Sigma^{1/2}(X)*\\epsilon)]-\\mathcal{D}\\left[Q(z|X)\\|P(z)\\right]\\right].\n",
    "$$"
   ]
<<<<<<< Updated upstream
=======
  },
  {
   "cell_type": "markdown",
   "id": "fa2bc640",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Code examples using Keras\n",
    "\n",
    "Code taken from  <https://keras.io/examples/generative/vae/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcaca0bb",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Variational AutoEncoder\n",
    "Author: [fchollet](https://twitter.com/fchollet)\n",
    "Date created: 2020/05/03\n",
    "Last modified: 2023/11/22\n",
    "Description: Convolutional Variational AutoEncoder (VAE) trained on MNIST digits.\n",
    "Accelerator: GPU\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "\"\"\"\n",
    "## Create a sampling layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Build the encoder\n",
    "\"\"\"\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "\"\"\"\n",
    "## Build the decoder\n",
    "\"\"\"\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "\"\"\"\n",
    "## Define the VAE as a `Model` with a custom `train_step`\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction),\n",
    "                    axis=(1, 2),\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Train the VAE\n",
    "\"\"\"\n",
    "\n",
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(mnist_digits, epochs=30, batch_size=128)\n",
    "\n",
    "\"\"\"\n",
    "## Display a grid of sampled digits\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_latent_space(vae, n=30, figsize=15):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    scale = 1.0\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = vae.decoder.predict(z_sample, verbose=0)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent_space(vae)\n",
    "\n",
    "\"\"\"\n",
    "## Display how the latent space clusters different digit classes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_label_clusters(vae, data, labels):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data, verbose=0)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "\n",
    "plot_label_clusters(vae, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa4cda",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Code in PyTorch for VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d9e17e-635a-430d-a65d-71d139f5ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import distributions\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, H)\n",
    "        self.enc_mu = torch.nn.Linear(H, latent_size)\n",
    "        self.enc_log_sigma = torch.nn.Linear(H, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mu = self.enc_mu(x)\n",
    "        log_sigma = self.enc_log_sigma(x)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        return torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = torch.tanh(self.linear2(x))\n",
    "        return torch.distributions.Normal(mu, torch.ones_like(mu))\n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, state):\n",
    "        q_z = self.encoder(state)\n",
    "        z = q_z.rsample()\n",
    "        return self.decoder(z), q_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc8c840f-0791-46f8-affb-8890ab149a48",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  60000\n",
      "0 742.6677856445312 -738.50146484375 4.166323661804199\n",
      "1 739.9192504882812 -734.99169921875 4.927535057067871\n",
      "2 740.1946411132812 -734.9848022460938 5.209833145141602\n",
      "3 740.029541015625 -734.5819702148438 5.447558879852295\n",
      "4 740.6784057617188 -734.83935546875 5.839040756225586\n",
      "5 739.3085327148438 -733.962890625 5.345613956451416\n",
      "6 739.6077270507812 -733.66064453125 5.94706916809082\n",
      "7 739.7880249023438 -733.751708984375 6.036327838897705\n",
      "8 740.5966186523438 -734.3396606445312 6.256959438323975\n",
      "9 739.818359375 -734.3383178710938 5.480045318603516\n",
      "10 738.6378784179688 -732.944580078125 5.693328380584717\n",
      "11 739.2891235351562 -733.3831176757812 5.9059906005859375\n",
      "12 739.7694091796875 -733.9091186523438 5.860269069671631\n",
      "13 739.2954711914062 -733.3406372070312 5.9548420906066895\n",
      "14 740.3397827148438 -734.0112915039062 6.328485012054443\n",
      "15 739.2918701171875 -733.5347290039062 5.757171630859375\n",
      "16 739.3868408203125 -733.532470703125 5.854349613189697\n",
      "17 738.8614501953125 -732.7805786132812 6.080869197845459\n",
      "18 739.5079345703125 -733.5432739257812 5.964648723602295\n",
      "19 740.2506713867188 -734.1340942382812 6.116603374481201\n",
      "20 739.4613037109375 -733.1238403320312 6.33746862411499\n",
      "21 740.4052734375 -734.1500854492188 6.25517463684082\n",
      "22 740.6605834960938 -734.7001953125 5.960386276245117\n",
      "23 739.2512817382812 -733.092041015625 6.159235000610352\n",
      "24 738.1182250976562 -732.1366577148438 5.981573104858398\n",
      "25 739.0986328125 -733.076904296875 6.021738052368164\n",
      "26 740.018798828125 -734.1741333007812 5.844646453857422\n",
      "27 739.6159057617188 -733.5619506835938 6.053940296173096\n",
      "28 739.1203002929688 -732.8870239257812 6.233285427093506\n",
      "29 739.588134765625 -733.43017578125 6.157947540283203\n",
      "30 740.0614013671875 -733.57080078125 6.490570068359375\n",
      "31 737.9950561523438 -731.8488159179688 6.146236419677734\n",
      "32 739.1751098632812 -733.0005493164062 6.174545764923096\n",
      "33 739.0252685546875 -732.8024291992188 6.222859859466553\n",
      "34 738.8564453125 -732.679443359375 6.176998138427734\n",
      "35 740.0811157226562 -733.80419921875 6.276892185211182\n",
      "36 738.6142578125 -732.5556640625 6.05862283706665\n",
      "37 739.9976806640625 -733.6718139648438 6.325886249542236\n",
      "38 739.6903076171875 -733.284423828125 6.405862808227539\n",
      "39 739.8013305664062 -733.7616577148438 6.039684772491455\n",
      "40 739.312744140625 -733.2646484375 6.0481038093566895\n",
      "41 739.2054443359375 -733.026123046875 6.179332256317139\n",
      "42 739.6060180664062 -733.25244140625 6.353586673736572\n",
      "43 740.3284912109375 -733.928955078125 6.399540424346924\n",
      "44 738.6707763671875 -732.4133911132812 6.2573676109313965\n",
      "45 738.0336303710938 -731.978515625 6.055117130279541\n",
      "46 739.6699829101562 -733.3909301757812 6.279026031494141\n",
      "47 739.9077758789062 -733.7634887695312 6.144310474395752\n",
      "48 738.626708984375 -732.2947387695312 6.331967830657959\n",
      "49 738.4725952148438 -732.5311889648438 5.941408157348633\n",
      "50 739.0667114257812 -733.202392578125 5.864328384399414\n",
      "51 739.3428344726562 -733.3799438476562 5.962911605834961\n",
      "52 738.951416015625 -732.8973999023438 6.05402135848999\n",
      "53 739.5658569335938 -733.2815551757812 6.284303188323975\n",
      "54 739.6079711914062 -733.1489868164062 6.4589715003967285\n",
      "55 739.2854614257812 -732.732421875 6.553016185760498\n",
      "56 738.8695678710938 -732.5770874023438 6.292454242706299\n",
      "57 739.9070434570312 -733.5829467773438 6.324125289916992\n",
      "58 739.6898803710938 -733.5234375 6.166437149047852\n",
      "59 738.65185546875 -732.4811401367188 6.170736789703369\n",
      "60 739.4676513671875 -733.105224609375 6.36245584487915\n",
      "61 740.12060546875 -733.7927856445312 6.327800750732422\n",
      "62 739.0076293945312 -732.549072265625 6.4585490226745605\n",
      "63 739.5352783203125 -733.18798828125 6.347280025482178\n",
      "64 738.8569946289062 -732.7178344726562 6.139143466949463\n",
      "65 739.0029296875 -732.523193359375 6.4797539710998535\n",
      "66 738.1939086914062 -732.0848999023438 6.109037399291992\n",
      "67 739.3861694335938 -733.1998901367188 6.18628454208374\n",
      "68 739.3423461914062 -733.1521606445312 6.19019079208374\n",
      "69 739.1702880859375 -732.88330078125 6.287008285522461\n",
      "70 738.0028076171875 -731.8027954101562 6.200003147125244\n",
      "71 739.1156616210938 -732.5420532226562 6.573592662811279\n",
      "72 739.5393676757812 -733.125 6.414371490478516\n",
      "73 738.9261474609375 -732.531982421875 6.394155979156494\n",
      "74 739.8997802734375 -733.5608520507812 6.33894681930542\n",
      "75 739.0664672851562 -732.9144897460938 6.151958465576172\n",
      "76 739.1373291015625 -732.708740234375 6.428600788116455\n",
      "77 739.8433227539062 -733.328857421875 6.51447057723999\n",
      "78 738.8466796875 -732.626708984375 6.219950199127197\n",
      "79 739.2030639648438 -732.9591674804688 6.243892669677734\n",
      "80 738.7994995117188 -732.6619262695312 6.137553691864014\n",
      "81 739.133544921875 -732.6786499023438 6.454873561859131\n",
      "82 738.73193359375 -732.3289184570312 6.4030375480651855\n",
      "83 740.3780517578125 -733.7228393554688 6.655214786529541\n",
      "84 739.7244262695312 -733.07861328125 6.6457839012146\n",
      "85 738.0996704101562 -731.6498413085938 6.4498209953308105\n",
      "86 738.7412719726562 -732.5355834960938 6.205715179443359\n",
      "87 739.368896484375 -733.048583984375 6.320332050323486\n",
      "88 739.6190185546875 -733.4032592773438 6.2157883644104\n",
      "89 737.8131103515625 -731.640869140625 6.172246932983398\n",
      "90 738.4564819335938 -732.01025390625 6.446216106414795\n",
      "91 739.3871459960938 -733.2311401367188 6.15601110458374\n",
      "92 738.9970703125 -732.6978149414062 6.299280166625977\n",
      "93 739.8682250976562 -733.5239868164062 6.3442206382751465\n",
      "94 739.6939086914062 -733.0939331054688 6.599971771240234\n",
      "95 739.1782836914062 -732.9473266601562 6.230977535247803\n",
      "96 739.8767700195312 -733.49462890625 6.382151126861572\n",
      "97 738.6400756835938 -732.2047729492188 6.435278415679932\n",
      "98 739.2304077148438 -733.0142211914062 6.216198444366455\n",
      "99 738.4749755859375 -732.3416137695312 6.133386135101318\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     # Normalize the images to be -0.5, 0.5\n",
    "     transforms.Normalize(0.5, 1)]\n",
    "    )\n",
    "mnist = torchvision.datasets.MNIST('./', download=True, transform=transform)\n",
    "\n",
    "input_dim = 28 * 28\n",
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "hidden_size = 512\n",
    "latent_size = 8\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    mnist, batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    pin_memory=torch.cuda.is_available())\n",
    "\n",
    "print('Number of samples: ', len(mnist))\n",
    "\n",
    "encoder = Encoder(input_dim, hidden_size, latent_size)\n",
    "decoder = Decoder(latent_size, hidden_size, input_dim)\n",
    "\n",
    "vae = VAE(encoder, decoder).to(device)\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        inputs, _ = data\n",
    "        inputs = inputs.view(-1, input_dim).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        p_x, q_z = vae(inputs)\n",
    "        log_likelihood = p_x.log_prob(inputs).sum(-1).mean()\n",
    "        kl = torch.distributions.kl_divergence(\n",
    "            q_z, \n",
    "            torch.distributions.Normal(0, 1.)\n",
    "        ).sum(-1).mean()\n",
    "        loss = -(log_likelihood - kl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l = loss.item()\n",
    "    print(epoch, l, log_likelihood.item(), kl.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "713fd889-0ea3-4a51-83e3-e953c58d6954",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_SingleProcessDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Visualize Reconstruction\u001b[39;00m\n\u001b[0;32m     16\u001b[0m data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(dataloader)\n\u001b[1;32m---> 17\u001b[0m images, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdata_iter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m()\n\u001b[0;32m     18\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_SingleProcessDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "# Function to visualize original vs reconstructed images\n",
    "def visualize_reconstruction(original_images, reconstructed_images, num_samples=5):\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 2*num_samples))\n",
    "    for i in range(num_samples):\n",
    "        axes[i, 0].imshow(original_images[i], cmap='gray')\n",
    "        axes[i, 0].set_title('Original')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(reconstructed_images[i], cmap='gray')\n",
    "        axes[i, 1].set_title('Reconstructed')\n",
    "        axes[i, 1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize Reconstruction\n",
    "data_iter = iter(dataloader)\n",
    "images, _ = data_iter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_images, _, _ = VAE(torch.tensor(images, dtype=torch.float).to(device))\n",
    "    reconstructed_images = reconstructed_images.cpu().numpy().reshape(-1, 28, 28)\n",
    "\n",
    "visualize_reconstruction(images, reconstructed_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da59cf0-6653-4bcb-95af-a874ae2ea199",
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
